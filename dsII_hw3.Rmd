---
title: "Data Science II: HW3"
author:
- "Shayne Estill"
date: "04/01/2025"
output: html_document
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)

```

# Libraries

```{r}

# Load libraries
library(tidyverse)
library(caret)
library(ggplot2)  
library(patchwork)
library(corrplot)
library(mgcv)
library(tidymodels)
library(earth)
library(boot) 
library(table1)
library(knitr)
library(pls)

```

In this problem, you will develop a model to predict whether a given car gets high or
low gas mileage based on the dataset “auto.csv”. The dataset contains 392 observations.
The response variable is “mpg cat”, which indicates whether the miles per gallon of a car
is high or low. The predictors include both continuous and categorical variables:
• cylinders: Number of cylinders between 4 and 8
• displacement: Engine displacement (cu. inches)
• horsepower: Engine horsepower
• weight: Vehicle weight (lbs.)
• acceleration: Time to accelerate from 0 to 60 mph (sec.)
• year: Model year (modulo 100)
• origin: Origin of car (1. American, 2. European, 3. Japanese)

Split the dataset into two parts: training data (70%) and test data (30%).

```{r}
# Load auto data
auto_data = read_csv(file = "/Users/shayneestill/Desktop/Data Science II/dsII_hw3/auto.csv", 
                        na = c("NA", ".", "")) |>
                        janitor::clean_names()
drop_na(auto_data)
set.seed(0401)

data_split <- initial_split(auto_data, prop = 0.8)

# Extract the training and test data
training_data <- training(data_split)
testing_data <- testing(data_split)

```


(a) Perform logistic regression analysis. Are there redundant predictors in your model?
If so, identify them. If there are none, please provide an explanation.

```{r}

```


(b) Train a multivariate adaptive regression spline (MARS) model. Does the MARS
model improve prediction performance compared to logistic regression?

```{r}

```


(c) Perform linear discriminant analysis using the training data. Plot the linear discriminant(s).

```{r}

```


(d) Which model will you choose to predict the response variable? Plot its ROC curve
and report the AUC. Next, select a probability threshold to classify observations and
compute the confusion matrix. Briefly interpret what the confusion matrix indicates
about your model’s performance.

```{r}

```



















# Data Cleaning and Preparation

```{r}

# Load training data
load("dat1.RData") 
dat1 <- dat1

# Load test data (dat2)
load("dat2.RData") 
dat2 <- dat2

# Set seed for reproducibility
set.seed(299)

data_split <- initial_split(dat1, prop = 0.8)

# Set 10-fold cross-validation
ctrl1 <- trainControl(method = "cv", number = 10)

# Remove non-predictor variables
training_data <- training(data_split) %>% select(-id) # remove ID variable 
testing_data <- testing(data_split) %>% select(-id) # remove ID variable 
dat2 <- dat2 %>% select(-id) # remove ID variable 

# Convert categorical variables to factor: Training Data
training_data <- training_data %>%
  mutate(gender = factor(gender, 
                         levels = c(0, 1), 
                         labels = c("Female", "Male")), 
    race = factor(as.character(race), 
                  levels = c("1", "2", "3", "4"), 
                  labels = c("White", "Asian", "Black", "Hispanic")),
    smoking = factor(as.character(smoking), levels = c("0", "1", "2"), 
                     labels = c("Never", "Former", "Current")),
    diabetes = factor(diabetes, 
                      levels = c(0, 1), 
                      labels = c("No", "Yes")),
    hypertension = factor(hypertension, 
                          levels = c(0, 1), 
                          labels = c("No", "Yes")))

# Convert categorical variables to factor: Testing Data
testing_data <- testing_data %>%
  mutate(gender = factor(gender, 
                         levels = c(0, 1), 
                         labels = c("Female", "Male")), 
    race = factor(as.character(race), 
                  levels = c("1", "2", "3", "4"), 
                  labels = c("White", "Asian", "Black", "Hispanic")),
    smoking = factor(as.character(smoking), levels = c("0", "1", "2"), 
                     labels = c("Never", "Former", "Current")),
    diabetes = factor(diabetes, 
                      levels = c(0, 1), 
                      labels = c("No", "Yes")),
    hypertension = factor(hypertension, 
                          levels = c(0, 1), 
                          labels = c("No", "Yes")))

# Convert categorical variables to factor: Dat2

dat2 <- dat2 %>%
  mutate(gender = factor(gender, 
                         levels = c(0, 1), 
                         labels = c("Female", "Male")), 
    race = factor(as.character(race), 
                  levels = c("1", "2", "3", "4"), 
                  labels = c("White", "Asian", "Black", "Hispanic")),
    smoking = factor(as.character(smoking), levels = c("0", "1", "2"), 
                     labels = c("Never", "Former", "Current")),
    diabetes = factor(diabetes, 
                      levels = c(0, 1), 
                      labels = c("No", "Yes")),
    hypertension = factor(hypertension, 
                          levels = c(0, 1), 
                          labels = c("No", "Yes")))

# Check structure of dataset
str(training_data)

```

# Exploratory Analysis

```{r}

## Set the plotting theme
theme1 <- trellis.par.get()
theme1$plot.symbol$col <- rgb(.2, .4, .2, .5)
theme1$plot.symbol$pch <- 16
theme1$plot.line$col <- rgb(.8, .1, .1, 1)
theme1$plot.line$lwd <- 2
theme1$strip.background$col <- rgb(.0, .2, .6, .2)
trellis.par.set(theme1)

## Feature Plots for numeric predictors (L2)
featurePlot(
  x = training_data[, c("age", "height", "weight", "bmi", "SBP", "LDL", "time")],
  y = training_data$log_antibody,
  plot = "scatter",
  span = 0.5,
  labels = c("Predictors", "Log Antibody"),
  type = c("p", "smooth"),
  layout = c(3, 2)
)

## Histograms for numeric predictors (need to ref code)

# Age histogram
h1 <- ggplot(training_data, aes(x = age)) +
  geom_histogram(binwidth = 1, color = "darkblue", fill = "lightblue") +
  ggtitle("Age Distribution") +
  theme(plot.title = element_text(hjust = 0.5))

# BMI histogram
h2 <- ggplot(training_data, aes(x = bmi)) +
  geom_histogram(binwidth = 1, color = "darkblue", fill = "lightblue") +
  ggtitle("BMI") +
  theme(plot.title = element_text(hjust = 0.5))

# SBP histogram
h3 <- ggplot(training_data, aes(x = SBP)) +
  geom_histogram(binwidth = 2, color = "darkblue", fill = "lightblue") +
  ggtitle("Systolic") +
  theme(plot.title = element_text(hjust = 0.5))

# LDL histogram
h4 <- ggplot(training_data, aes(x = LDL)) +
  geom_histogram(binwidth = 5, color = "darkblue", fill = "lightblue") +
  ggtitle("LDL") +
  theme(plot.title = element_text(hjust = 0.5))

# Time since vaccination histogram
h5 <- ggplot(training_data, aes(x = time)) +
  geom_histogram(binwidth = 5, color = "darkblue", fill = "lightblue") +
  ggtitle("Time Since Vaccination") +
  theme(plot.title = element_text(hjust = 0.5))

# Combine using patchwork
(h1 + h2) / (h3 + h4) / (h5 + plot_spacer())


## Boxplots for categorical predictors

p1 <- ggplot(training_data, aes(x = factor(gender), y = log_antibody)) +
  geom_boxplot() +
  labs(x = "Gender (0 = Female, 1 = Male)", y = NULL) +
  theme_bw()

p2 <- ggplot(training_data, aes(x = smoking, y = log_antibody)) +
  geom_boxplot() +
  labs(x = "Smoking Status", y = NULL) +
  theme_bw()

p3 <- ggplot(training_data, aes(x = race, y = log_antibody)) +
  geom_boxplot() +
  labs(x = "Race", y = NULL) +
  theme_bw()

p4 <- ggplot(training_data, aes(x = factor(diabetes), y = log_antibody)) +
  geom_boxplot() +
  labs(x = "Diabetes", y = NULL) +
  theme_bw()

p5 <- ggplot(training_data, aes(x = factor(hypertension), y = log_antibody)) +
  geom_boxplot() +
  labs(x = "Hypertension", y = NULL) +
  theme_bw()

# Using patchwork
((p1 + p2) / (p3 + p4) / (p5 + plot_spacer())) 

## Correlation Plot

# Matrix of predictors 
x <- model.matrix(log_antibody ~ ., training_data)[, -1]

# Vector of response
y <- training_data$log_antibody

# Produce corrplot
corrplot::corrplot(cor(x), method = 'circle', type = 'full')

```

Looking at the correlation plot we see a few predictors are correlated, for example weight and bmi and height and bmi seem to have a decently strong correlation. SBP and hypertension also have correlation. These correlations make sense as BMI uses height and weight to calculate the score, and SBP and hypertension status are linked. Keeping in mind that these correlations among predictors may cause multicollinearity we should consider regularization techniques (lasso/ridge penalties) in our models. 


```{r table1}
# https://cran.r-project.org/web/packages/table1/vignettes/table1-examples.html 

label(training_data$height) <- "Height (cm)"
label(training_data$age) <- "Age (yrs)"
label(training_data$weight) <- "Weight (kgs)"
label(training_data$bmi) <- "Body Mass Index (weight/ (height)^2)"
label(training_data$SBP) <- "Systolic blood pressure (mmHg)"
label(training_data$LDL) <- "LDL cholestrol (mg/dL)"
label(training_data$time) <- "Time since vaccination (in days)"
label(training_data$gender) <- "Gender"
label(training_data$race) <- "Race"
label(training_data$smoking) <- "Smoking status"
label(training_data$diabetes) <- "Diabetes"
label(training_data$hypertension) <- "Hypertension"



table1= table1(~ gender + race + age + height + weight + bmi + SBP + LDL+ time + 
         smoking + diabetes + hypertension, data=training_data)
knitr::kable(table1)

```


Based on the exploratory analysis of continuous predictors, time since vaccination appears to be right-skewed.

# Model Selection

## Linear Regression
```{r}
# Set seed for reproducibility
set.seed(299)

# linear regression
ctrl1 <- trainControl(method = "cv", number = 10)

# Fit linear model
lm.fit <- train(log_antibody ~ ., 
                data = training_data, 
                method = "lm", 
                trControl = ctrl1)

# model summary
summary_model <- summary(lm.fit$finalModel)
print(summary_model)

```
The first model we will try is multiple linear regression. This strength of this model lays in its simplicity and interpretability, so if later we find it performs the best it would be the ideal choice. However, from our EDA earlier we already have an idea that a linear model may not be the best since none of the pdps show a clear linear trend implying there may be more complexities to consider. Also, as shown in our correlation matrix, there may be correlation among predictors (e.g. height/weight and bmi) which we may want to consider through use of more complex models that have regularization penalities. The summary shows us the significant predictors in our multiple linear regression are age, gender, smoking status, height, weight and bmi. 

## Elastic Net

```{r}
set.seed(299)
enet_fit <- train(log_antibody ~ ., data = training_data, method = "glmnet",
            tuneGrid = expand.grid(alpha = seq(0, 1, length = 21),
            lambda = exp(seq(-3, 3, length = 100))),
            trControl = ctrl1)

enet_fit$bestTune

# Checking to see if it is in the grid or not
best_lambda <- enet_fit$bestTune$lambda
lambda_range <- range(enet_fit$results$lambda)

print(best_lambda)
print(format(lambda_range, scientific = FALSE))

myCol <- rainbow(25)
myPar <- list(superpose.symbol = list(col = myCol),
              superpose.line = list(col = myCol))

plot(enet_fit, par.settings = myPar)

plot(enet_fit, xvar = "lambda", label = TRUE)

```

*add justification for grid --> tried a range of lambda values and the lambda was at the lower
range of the grid...The optimal lambda was found to be at the lower bound 
The r-squared value is relatively low, thus providing evidence that this may not be a great model. 

## PCR

```{r}
set.seed(299)
pcr_mod <- pcr(log_antibody ~ .,
data = training_data,
scale = TRUE, # scale = FALSE by default
validation = "CV")
summary(pcr_mod)


# plot cross-validated mean squared error (MSEP)
validationplot(pcr_mod, val.type = "MSEP", legendpos = "topright")
```

```{r}
# determine the optimal number of components
cv_mse <- RMSEP(pcr_mod)
ncomp_cv <- which.min(cv_mse$val[1,,]) - 1
ncomp_cv
```

## PCR

```{r}

set.seed(299)
pcr_fit <- train(x, y,
                 method = "pcr",
                 tuneGrid = data.frame(ncomp = 1:16),
                 trControl = ctrl1,
                 preProcess = c("center", "scale"))

# Plot
ggplot(pcr_fit, highlight = TRUE) + theme_bw()

```

We fit the PCR model using the function pcr(). PCR and PLS models are good for addressing
multicollinearity by maximizing the variance of predictor variables. 

## PLS  

```{r}

set.seed(299)

# Checking initial grid range while fitting PLS model
# pls_fit <- train(
#  x = x,
#  y = y,
#  method = "pls",
#  tuneGrid = data.frame(ncomp = 1:8),
#  trControl = ctrl1,
 # preProcess = c("center", "scale")
#)

# Expanded grid to check more 
pls_fit <- train(
  x = x,
  y = y,
  method = "pls",
  tuneGrid = data.frame(ncomp = 1:13),
  trControl = ctrl1,
  preProcess = c("center", "scale")
)

# Summary of model
summary(pls_fit)

# Visualize
ggplot(pls_fit, highlight = TRUE) 

```

We fit the PLS model using the function plsr(). This method uses a small number of 
the original inputs and considers out outcome, log antibody levels, making it more
predictive over PCR potentially. 

We fit an elastic net model using the train() with method = "glmnet" function. Elastic
net models are good for effectively dealing with highly correlated groups of predictors. 

**Justification about tuning grid** 

Initially, our tuning grid was set to 1:8 to see the range of values for RMSE. However, we expanded the tuning grid to ncomp = 1:13 to explore whether additional components could meaningfully reduce RMSE. While RMSE continued to decrease slightly, the performance plateaued after 9 components, with no further improvement beyond that. Therefore, we were able to confirm that our original grid was sufficient and that 9 components is an appropriate model choice without overfitting.

## MARS - Naomi

```{r}

# Set seed for reproducibility
set.seed(299)

# Specify MARS grid - first attempt 
# mars_grid <- expand.grid(
#  degree = 1:3, # degree of interactions
#  nprune = 2:15  # no. of retained terms
# )

# MARS tuning grid - expanding grid 
mars_grid <- expand.grid(
  degree = 1:4,     # interaction degrees
  nprune = 2:20     # number of terms
)

# Train MARS model to predict log_antibody
mars.fit <- train(log_antibody ~ .,
                  data = training_data,
                  method = "earth",
                  tuneGrid = mars_grid,
                  trControl = ctrl1)

# Plot CV performance
ggplot(mars.fit)


# Optimal parameters
mars.fit$bestTune

# Final model coefficients
mars_coef <- coef(mars.fit$finalModel)

mars_coef

```

To evaluate the optimal complexity of the MARS model, I performed a grid search across degrees 1 to 4 and a range of 2 to 20 terms (`nprune`). Cross-validation results showed that models with degree = 1 consistently achieved the lowest RMSE, and increasing the interaction degree did not lead to further improvements. 

As the interaction `degree` increased, RMSE values became more variable rather than decreasing steadily, suggesting there was no gain in prediction accuracy. The `nprune` range of 2 to 20 also appeared appropriate — RMSE decreased initially with more terms but then plateaued, indicating that adding further complexity would not significantly improve model performance. Based on this, I selected `degree = 1` as the optimal choice.

In other words, cross-validation results showed that a model with approximately 9 terms and no interactions (product degree = 1) minimized prediction error (RMSE ≈ 0.529). 

**Optimal parameters and final model coefficients**
The best-tuned model selected `degree = 1` and `nprune = `10`. Therefore, the final model includes 10 retained terms with no interaction effects.

## GAM

I will proceed with constructing a GAM model, allowing us to mix non-linear and linear terms and build a model estimating the relationship between the outcome (`log_antibody`) and predictors in the provided dataset.
 
```{r}

# Set seed for reproducibility
set.seed(299)

# Fit a GAM model, using training data
gam_antibody <- gam(log_antibody ~ gender + race + 
                      smoking + s(age) +
                      s(height) + s(weight) + s(bmi) +
                      diabetes + hypertension + s(SBP) + 
                      s(LDL) + s(time), 
                    data = training_data)

# Plot smooth GAM terms
# Code source: https://bookdown.org/ndphillips/YaRrr/arranging-plots-with-parmfrow-and-layout.html
par(mfrow = c(3, 2))  
plot(gam_antibody)
par(mfrow = c(1, 1)) # Reset plotting window


# Use train() from caret to fit GAM Model

gam.fit <- train(
  log_antibody ~ gender + race + 
    smoking + age +
    height + weight + bmi + diabetes + 
    hypertension + SBP + 
    LDL + time,
  data = training_data,
  method = "gam",
  trControl = ctrl1
)

# Predictor EDF values from mgcv fitted GAM model
summary(gam_antibody)

```

* Some predictors have estimated degrees of freedom (edf) greater than 1, specifically height, weight, BMI, and time,  indicating they are modeled as nonlinear smooth functions. This is supported by the GAM predictor plots. Weight, while slightly above 1, was not statistically significant, and does not show strong evidence of nonlinearity.

* Age, SBP, and LDL each have an edf of exactly 1.00, indicating that they are modeled as approximately linear. Of these, only age was statistically significant (p < 2e-16), suggesting a strong linear association with log antibody levels.

* Among the nonlinear terms, height, BMI, and time were statistically significant (p < 0.05), indicating meaningful nonlinear relationships with the outcome.


# Comparison/Model selection

```{r}

# Compare models using cross-validated RMSE
resamp <- resamples(list(
  linear = lm.fit,
  pls = pls_fit,
  pcr = pcr_fit,
  elastic_net = enet_fit,
  mars = mars.fit,
  gam = gam.fit))

# Print CV summary
summary(resamp)

# Plot RMSE comparison
bwplot(resamp, metric = "RMSE")

# Show in table format
knitr::kable(
  tibble::rownames_to_column(
    data.frame("Mean RMSE" = round(summary(resamp)$statistics$RMSE[, "Mean"], 5),
               check.names = FALSE),
    var = "Model"), 
  caption = "Mean RMSE for Each Model (10-fold CV)")

```

MARS has the smallest mean RMSE (`0.5284505`), therefore we select it as the optimal model.

We will first evaluate its performance based on the testing dataset.

```{r}

# Set seed for reproducibility
set.seed(299)

# Obtain response variable from testing data
y_testing_MARS <- testing_data$log_antibody

# Predict on test data using trained MARS model
y_pred_MARS <- predict(mars.fit, newdata = testing_data)

# Compute RMSE (Test Error)
test_rmse <- sqrt(mean((y_testing_MARS - y_pred_MARS)^2))

# Print test RMSE
test_rmse

```

Therefore, the test error for the MARS model is `0.5269679`.

# Test Error for Selected Model Applied to New Dataset: MARS

Subsequently, we want to evaluate the generalizability of this model using another independent dataset, (`dat2`)`.

```{r}

# Set seed for reproducibility
set.seed(299)

# Obtain response variable from dat2
y_testing_DAT2 <- dat2$log_antibody

# Predict on dat2 using trained MARS model
y_pred_DAT2 <- predict(mars.fit, newdata = dat2)

# Compute RMSE (Test Error)
test_rmse <- sqrt(mean((y_testing_DAT2 - y_pred_DAT2)^2))

# Print test RMSE
test_rmse

```

The test error for the MARS model when fit on the new dataset (`dat2`) is `0.5335579`. 
